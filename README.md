# Evaluating and Debugging Generative AI


## üëã Welcome! 
Welcome to the "Evaluating and Debugging Generative AI" GitHub repository! This repository serves as a comprehensive resource for understanding, evaluating, and debugging generative artificial intelligence (AI) models. 
Generative AI has made significant advancements in various fields, including natural language processing, computer vision, and more. However, effectively evaluating and debugging these models can be challenging due to their complexity.

## ‚≠ê Before getting started:

    üç¥ To incorporate this repository into your own project, you can fork it.
    ‚≠ê In order to receive notifications about future improvements, please consider starring this repository.

## üìö Repo structure
+ [`CheatSheets`](/CheatSheets/): Quick reference materials for different concepts or techniques
+ [`Data`](/Data/): Data files for analysis or model training
+ [`Images`](/Images/): Images of various objects, scenes, or entities from/for analysis.
+ [`Notebooks`](/Notebooks/): Code files for different projects
+ [`Resources`](/Resources/): Supporting materials or references for the project

## üìñ Contents 
|  Name  |  Description  |  URL  |Technologies | Industry/Domain |
| :-----:| :------------:| :----:|:----:|:----:|
| **Track experiments**| Rapid experimentation is fundamental to machine learning. In this tutorial, we use W&B to track and visualize experiments so that we can quickly iterate and understand our results.  | [GitHub](https://github.com/natnew/Evaluating-and-Debugging-Generative-AI)| ![Static Badge](https://img.shields.io/badge/Python%20-%20black) | XXX 
| **Visualize predictions**|This covers how to track, visualize, and compare model predictions over the course of training, using PyTorch on MNIST data.  | [GitHub](https://github.com/natnew/Evaluating-and-Debugging-Generative-AI)| ![Static Badge](https://img.shields.io/badge/Python%20-%20black) | XXX 
| **Tune hyperparameters**|Searching through high dimensional hyperparameter spaces to find the most performant model can get unwieldy very fast. Hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the most accurate model.   | [GitHub](https://github.com/natnew/Evaluating-and-Debugging-Generative-AI)| ![Static Badge](https://img.shields.io/badge/Python%20-%20black) | XXX 
| **Track models and datasets**| In this notebook, we'll show you how to track your ML experiment pipelines using W&B Artifacts.  | [GitHub](https://github.com/natnew/Evaluating-and-Debugging-Generative-AI)| ![Static Badge](https://img.shields.io/badge/Python%20-%20black) | XXX 
| **Register models**|The model registry is a central place to house and organize all the model tasks and their associated artifacts being worked on across an org.  | [GitHub](https://github.com/natnew/Evaluating-and-Debugging-Generative-AI)| ![Static Badge](https://img.shields.io/badge/Python%20-%20black) | XXX 
| **Iterate on LLMs**|Use W&B Prompts to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.  | [GitHub](https://github.com/natnew/Evaluating-and-Debugging-Generative-AI)| ![Static Badge](https://img.shields.io/badge/Python%20-%20black) | XXX 

## Setup



## License

See the [LICENSE](LICENSE.md) file for license rights and limitations (MIT).
